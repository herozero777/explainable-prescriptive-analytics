{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess Event Logs for Process Prediction (Process Mining)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import tqdm\n",
    "pd.options.display.max_columns= None\n",
    "if \"src\" in os.getcwd():\n",
    "    os.chdir(\"../\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:23.703513832Z",
     "start_time": "2023-06-19T10:50:23.267569456Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class EventLog:\n",
    "    def __init__(self, file_path, kpi, dataset):\n",
    "        self.file_path = file_path\n",
    "        self.kpi = kpi\n",
    "        self.dataset = dataset\n",
    "        # === read file to get the dataframe\n",
    "        if dataset == \"completed.csv\":\n",
    "            self.df = pd.read_csv(self.file_path)\n",
    "            self.df[start_date_name] = pd.to_numeric( self.df[start_date_name] ) // 1_000  # Convert Epochs from milliseconds to seconds\n",
    "            self.df[end_date_name] = pd.to_numeric( self.df[end_date_name] ) // 1_000  # Convert Epochs from milliseconds to seconds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:23.841683704Z",
     "start_time": "2023-06-19T10:50:23.835320244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Select the dataset to use\n",
    "# data_file_path = \"./data/VINST_cases_incidents_mini.csv\"\n",
    "# data_file_path = \"./data/VINST cases incidents.csv\"\n",
    "# dataset = \"VINST cases incidents.csv\"  # VINST case incident, for Total Time KPI optimization\n",
    "dataset = \"completed.csv\"                # bank_account_closure, for Activity Occurrence KPI optimization"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:24.114478935Z",
     "start_time": "2023-06-19T10:50:24.108246500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "data_file_path = os.path.join(data_dir, dataset)\n",
    "if dataset == \"completed.csv\":\n",
    "    KPI = \"activity_occurrence\"\n",
    "    case_id_name = \"REQUEST_ID\"\n",
    "    start_date_name = \"START_DATE\"\n",
    "    activity_column_name = \"ACTIVITY\"\n",
    "    end_date_name = \"END_DATE\"\n",
    "    activity_to_avoid = \"Back-Office Adjustment Requested\"\n",
    "    train_size = 0.8                  # Where the time split happens on the dataset\n",
    "    df = pd.read_csv(data_file_path)  # concern: what is date col position is different?\n",
    "    df[start_date_name] = pd.to_numeric( df[start_date_name] ) // 1_000  # Convert Epochs from milliseconds to seconds\n",
    "    df[end_date_name] = pd.to_numeric( df[end_date_name] ) // 1_000  # Convert Epochs from milliseconds to seconds\n",
    "\n",
    "else:\n",
    "    KPI = \"total_time\"\n",
    "    case_id_name = 'SR_Number'  # The case identifier column name.\n",
    "    start_date_name = 'Change_Date+Time'  # Maybe change to start_et (start even time)\n",
    "    activity_column_name = \"ACTIVITY\"\n",
    "    end_date_name = None\n",
    "    train_size = 0.8\n",
    "    df = pd.read_csv(data_file_path, parse_dates=[1])  # concern: what is date col position is different?\n",
    "    df[start_date_name] = df[start_date_name].view('int64') // 10**9  # 10**9 Converts nanoseconds to seconds\n",
    "\n",
    "preprocessed_data_dir = \"./preprocessed_datasets\"\n",
    "use_saved_model = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:24.678167751Z",
     "start_time": "2023-06-19T10:50:24.463976205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 212,721\n"
     ]
    },
    {
     "data": {
      "text/plain": "    REQUEST_ID   CLOSURE_TYPE   CLOSURE_REASON   \n0  20175000168  Client Recess  1 - Client lost  \\\n1  20175000168  Client Recess  1 - Client lost   \n2  20175000168  Client Recess  1 - Client lost   \n3  20175000168  Client Recess  1 - Client lost   \n4  20175000168  Client Recess  1 - Client lost   \n\n                                            ACTIVITY    END_DATE  START_DATE   \n0  Service closure Request with network responsib...  1539175786  1539175692  \\\n1     Service closure Request with BO responsibility  1539602104  1539175786   \n2            Pending Request for Reservation Closure  1539602345  1539602104   \n3                        Pending Liquidation Request  1539745391  1539602345   \n4             Request completed with account closure  1539745391  1539745391   \n\n   CE_UO         ROLE  \n0  00044    APPLICANT  \n1    BOC  BACK-OFFICE  \n2    BOC  BACK-OFFICE  \n3    BOC  BACK-OFFICE  \n4    BOC  BACK-OFFICE  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>REQUEST_ID</th>\n      <th>CLOSURE_TYPE</th>\n      <th>CLOSURE_REASON</th>\n      <th>ACTIVITY</th>\n      <th>END_DATE</th>\n      <th>START_DATE</th>\n      <th>CE_UO</th>\n      <th>ROLE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20175000168</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Service closure Request with network responsib...</td>\n      <td>1539175786</td>\n      <td>1539175692</td>\n      <td>00044</td>\n      <td>APPLICANT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20175000168</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Service closure Request with BO responsibility</td>\n      <td>1539602104</td>\n      <td>1539175786</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20175000168</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Pending Request for Reservation Closure</td>\n      <td>1539602345</td>\n      <td>1539602104</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20175000168</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Pending Liquidation Request</td>\n      <td>1539745391</td>\n      <td>1539602345</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20175000168</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Request completed with account closure</td>\n      <td>1539745391</td>\n      <td>1539745391</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of rows: {len(df):,}\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:26.270983617Z",
     "start_time": "2023-06-19T10:50:26.260580542Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def move_essential_columns(df, case_id_name: str, start_date_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Puts the column `case_id_name` and `start_date_name` in start and then the rest of the\n",
    "    columns\n",
    "    \"\"\"\n",
    "    columns = df.columns.to_list()\n",
    "    columns.remove(case_id_name)\n",
    "    columns.remove(start_date_name)\n",
    "    df = df[[case_id_name, start_date_name] + columns]\n",
    "    return df\n",
    "\n",
    "def sort_df_case_id_n_time(df, case_id_name: str, start_date_name: str) -> pd.DataFrame:\n",
    "    \"\"\" Sorts in ascending order according the case-ids and start-date time \"\"\"\n",
    "    df = df.sort_values([case_id_name, start_date_name], axis=0, ascending=True, kind='quicksort', na_position='last')\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:28.034419224Z",
     "start_time": "2023-06-19T10:50:28.026210903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df = df.fillna(\"missing\")  # Replace all NaN values with the keyword \"missing\n",
    "df = move_essential_columns(df, case_id_name, start_date_name)\n",
    "# Maybe use this or the function\n",
    "df = df.sort_values([case_id_name, start_date_name], axis=0, ascending=True, kind='quicksort', na_position='last')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:28.774793623Z",
     "start_time": "2023-06-19T10:50:28.564934043Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# === Fix Time Columns (temporary)\n",
    "# if dataset == \"completed.csv\":\n",
    "#     df[start_date_name] = pd.to_datetime( df[start_date_name], unit='s')\n",
    "#     df[end_date_name] = pd.to_datetime( df[end_date_name], unit='s')\n",
    "#     df.to_csv(\"bank-acc_v2.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:30.012072981Z",
     "start_time": "2023-06-19T10:50:30.000515131Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# ====== For developing attributes that are specific to an entire trace\n",
    "# gdf = df.groupby(case_id_name)\n",
    "# for case_id, group in gdf:\n",
    "#     print(group)\n",
    "#     if end_date_name is not None:\n",
    "#         print( group[end_date_name] - group[start_date_name] )\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:30.192753619Z",
     "start_time": "2023-06-19T10:50:30.185629429Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_time_from_midnight(dt_obj: pd.Timestamp) -> int:\n",
    "    \"\"\"\n",
    "    :param dt_obj: Is a datetime object.\n",
    "    Return seconds elapsed from midnight (or day start 00:00:00)\n",
    "    \"\"\"\n",
    "    return dt_obj.hour * 3600 + dt_obj.minute * 60 + dt_obj.second\n",
    "\n",
    "def add_features(df, case_id_name: str, start_date_name: str, end_date_name=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Times that are integer are in seconds.\n",
    "    \"\"\"\n",
    "    df[\"time_from_first\"] = 0\n",
    "    df[\"time_from_previous_et\"] = 0\n",
    "    df[\"time_from_midnight\"] = 0\n",
    "    df[\"weekday\"] = 0\n",
    "    if end_date_name is not None:\n",
    "        df[\"activity_duration\"] = 0\n",
    "    df[\"time_remaining\"] = 0\n",
    "\n",
    "    gdf = df.groupby(case_id_name)\n",
    "    i = 0\n",
    "    for case_id, group in gdf:\n",
    "\n",
    "        case_first_time = group[start_date_name].min()  # The timestamp at the first event of the 'case'\n",
    "        case_last_time = group[start_date_name].max()   # The timestamp of the last event of the 'case'.\n",
    "        # Note last event may not be the ending or final event e.g. a running case.\n",
    "\n",
    "        # Expression in brackets return a series, we apply the desired function on all elements using .apply(...)\n",
    "        time_from_first = group[start_date_name] - case_first_time\n",
    "\n",
    "        time_from_previous_et =  group[start_date_name].diff()\n",
    "        time_from_previous_et = time_from_previous_et.fillna(0)  # edge case where is first row has a zero\n",
    "\n",
    "        time_from_midnight = pd.to_datetime( group[start_date_name], unit='s' ).apply(lambda x: get_time_from_midnight(x))\n",
    "\n",
    "        # dt: Accessor object for datetimelike properties of the Series' values.\n",
    "        weekday_number = pd.to_datetime( group[start_date_name], unit='s' ).dt.dayofweek\n",
    "        weekday_str = weekday_number.replace({0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\",\n",
    "                                              3: \"Thursday\", 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"})\n",
    "        if end_date_name is not None:\n",
    "            activity_duration = group[end_date_name] - group[start_date_name]\n",
    "            df.loc[group.index, \"activity_duration\"] = activity_duration\n",
    "\n",
    "        time_remaining = case_last_time - group[start_date_name]\n",
    "\n",
    "        # # # Set values in the dataframe at the index corresponding to the group's case-id.\n",
    "        # Maybe possible to optimize.\n",
    "        # Possible way 1: Create a mini-df from group augment it with new features and at the end pd.concat all the mini-dfs\n",
    "        df.loc[group.index, \"time_from_first\"] = time_from_first\n",
    "        df.loc[group.index, \"time_from_previous_et\"] = time_from_previous_et\n",
    "        df.loc[group.index, \"time_from_midnight\"] = time_from_midnight\n",
    "        df.loc[group.index, \"weekday\"] = weekday_str\n",
    "        df.loc[group.index, \"time_remaining\"] = time_remaining\n",
    "\n",
    "    return df\n",
    "\"\"\"\n",
    "Given a trace we want to create all prefixes of it. E.g. trace: <a, b, c> then prefixes: {<a>, <a,b>, <a,b,c>}.\n",
    "Here the letters denote activities. Next, just like a trace has a KPI value, all its prefixes are associated\n",
    "with the same prefix value.\n",
    "\n",
    "This function implements the aggregated history version of creating prefixes from traces. The idea is that we\n",
    "create a different column for each activity, then within a trace (or case) the counter value keeps track of how many\n",
    "times an activity has occurred. View the before and after CSVs for a better understanding.\n",
    "\"\"\"\n",
    "\n",
    "def add_aggregated_history(df, case_id_name, activity_column_name) -> pd.DataFrame:\n",
    "\n",
    "    for activity in df[activity_column_name].unique():\n",
    "        df[f\"# {activity_column_name}={activity}\"] = 0\n",
    "        # first put 1 in correspondence to each activity\n",
    "        df.loc[df[activity_column_name] == activity, f\"# {activity_column_name}={activity}\"] = 1\n",
    "        # sum the count from the previous events\n",
    "        df[f\"# {activity_column_name}={activity}\"] = df.groupby(case_id_name)[f\"# {activity_column_name}={activity}\"].cumsum()\n",
    "    return df\n",
    "\n",
    "def change_history(df, activity_column_name=\"ACTIVITY\"):\n",
    "    for i, row in df.iterrows():\n",
    "        act = df.at[i, activity_column_name]\n",
    "        if df.at[i, '# ' + activity_column_name + '=' + act] != 0:\n",
    "            df.at[i, '# ' + activity_column_name + '=' + act] -= 1\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:50:30.418374304Z",
     "start_time": "2023-06-19T10:50:30.367690213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = add_features(df, case_id_name, start_date_name, end_date_name)\n",
    "df = add_aggregated_history(df, case_id_name, activity_column_name)\n",
    "df = change_history(df, activity_column_name=activity_column_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:39.698066937Z",
     "start_time": "2023-06-19T10:50:32.630982224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "    REQUEST_ID  START_DATE   CLOSURE_TYPE   CLOSURE_REASON   \n0  20175000168  1539175692  Client Recess  1 - Client lost  \\\n1  20175000168  1539175786  Client Recess  1 - Client lost   \n2  20175000168  1539602104  Client Recess  1 - Client lost   \n3  20175000168  1539602345  Client Recess  1 - Client lost   \n4  20175000168  1539745391  Client Recess  1 - Client lost   \n5  20175000642  1541672782    Bank Recess  1 - Client lost   \n\n                                            ACTIVITY    END_DATE  CE_UO   \n0  Service closure Request with network responsib...  1539175786  00044  \\\n1     Service closure Request with BO responsibility  1539602104    BOC   \n2            Pending Request for Reservation Closure  1539602345    BOC   \n3                        Pending Liquidation Request  1539745391    BOC   \n4             Request completed with account closure  1539745391    BOC   \n5                                    Request created  1541672852  00624   \n\n          ROLE  time_from_first  time_from_previous_et  time_from_midnight   \n0    APPLICANT                0                      0               46092  \\\n1  BACK-OFFICE               94                     94               46186   \n2  BACK-OFFICE           426412                 426318               40504   \n3  BACK-OFFICE           426653                    241               40745   \n4  BACK-OFFICE           569699                 143046               10991   \n5    APPLICANT                0                      0               37582   \n\n     weekday  activity_duration  time_remaining   \n0  Wednesday                 94          569699  \\\n1  Wednesday             426318          569605   \n2     Monday                241          143287   \n3     Monday             143046          143046   \n4  Wednesday                  0               0   \n5   Thursday                 70          578503   \n\n   # ACTIVITY=Service closure Request with network responsibility   \n0                                                  0               \\\n1                                                  1                \n2                                                  1                \n3                                                  1                \n4                                                  1                \n5                                                  0                \n\n   # ACTIVITY=Service closure Request with BO responsibility   \n0                                                  0          \\\n1                                                  0           \n2                                                  1           \n3                                                  1           \n4                                                  1           \n5                                                  0           \n\n   # ACTIVITY=Pending Request for Reservation Closure   \n0                                                  0   \\\n1                                                  0    \n2                                                  0    \n3                                                  1    \n4                                                  1    \n5                                                  0    \n\n   # ACTIVITY=Pending Liquidation Request   \n0                                       0  \\\n1                                       0   \n2                                       0   \n3                                       0   \n4                                       1   \n5                                       0   \n\n   # ACTIVITY=Request completed with account closure   \n0                                                  0  \\\n1                                                  0   \n2                                                  0   \n3                                                  0   \n4                                                  0   \n5                                                  0   \n\n   # ACTIVITY=Request created  # ACTIVITY=Authorization Requested   \n0                           0                                   0  \\\n1                           0                                   0   \n2                           0                                   0   \n3                           0                                   0   \n4                           0                                   0   \n5                           0                                   0   \n\n   # ACTIVITY=Evaluating Request (NO registered letter)   \n0                                                  0     \\\n1                                                  0      \n2                                                  0      \n3                                                  0      \n4                                                  0      \n5                                                  0      \n\n   # ACTIVITY=Network Adjustment Requested   \n0                                        0  \\\n1                                        0   \n2                                        0   \n3                                        0   \n4                                        0   \n5                                        0   \n\n   # ACTIVITY=Pending Request for acquittance of heirs   \n0                                                  0    \\\n1                                                  0     \n2                                                  0     \n3                                                  0     \n4                                                  0     \n5                                                  0     \n\n   # ACTIVITY=Request deleted  # ACTIVITY=Back-Office Adjustment Requested   \n0                           0                                            0  \\\n1                           0                                            0   \n2                           0                                            0   \n3                           0                                            0   \n4                           0                                            0   \n5                           0                                            0   \n\n   # ACTIVITY=Evaluating Request (WITH registered letter)   \n0                                                  0       \\\n1                                                  0        \n2                                                  0        \n3                                                  0        \n4                                                  0        \n5                                                  0        \n\n   # ACTIVITY=Request completed with customer recovery   \n0                                                  0    \\\n1                                                  0     \n2                                                  0     \n3                                                  0     \n4                                                  0     \n5                                                  0     \n\n   # ACTIVITY=Pending Request for Network Information  \n0                                                  0   \n1                                                  0   \n2                                                  0   \n3                                                  0   \n4                                                  0   \n5                                                  0   ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>REQUEST_ID</th>\n      <th>START_DATE</th>\n      <th>CLOSURE_TYPE</th>\n      <th>CLOSURE_REASON</th>\n      <th>ACTIVITY</th>\n      <th>END_DATE</th>\n      <th>CE_UO</th>\n      <th>ROLE</th>\n      <th>time_from_first</th>\n      <th>time_from_previous_et</th>\n      <th>time_from_midnight</th>\n      <th>weekday</th>\n      <th>activity_duration</th>\n      <th>time_remaining</th>\n      <th># ACTIVITY=Service closure Request with network responsibility</th>\n      <th># ACTIVITY=Service closure Request with BO responsibility</th>\n      <th># ACTIVITY=Pending Request for Reservation Closure</th>\n      <th># ACTIVITY=Pending Liquidation Request</th>\n      <th># ACTIVITY=Request completed with account closure</th>\n      <th># ACTIVITY=Request created</th>\n      <th># ACTIVITY=Authorization Requested</th>\n      <th># ACTIVITY=Evaluating Request (NO registered letter)</th>\n      <th># ACTIVITY=Network Adjustment Requested</th>\n      <th># ACTIVITY=Pending Request for acquittance of heirs</th>\n      <th># ACTIVITY=Request deleted</th>\n      <th># ACTIVITY=Back-Office Adjustment Requested</th>\n      <th># ACTIVITY=Evaluating Request (WITH registered letter)</th>\n      <th># ACTIVITY=Request completed with customer recovery</th>\n      <th># ACTIVITY=Pending Request for Network Information</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20175000168</td>\n      <td>1539175692</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Service closure Request with network responsib...</td>\n      <td>1539175786</td>\n      <td>00044</td>\n      <td>APPLICANT</td>\n      <td>0</td>\n      <td>0</td>\n      <td>46092</td>\n      <td>Wednesday</td>\n      <td>94</td>\n      <td>569699</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20175000168</td>\n      <td>1539175786</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Service closure Request with BO responsibility</td>\n      <td>1539602104</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n      <td>94</td>\n      <td>94</td>\n      <td>46186</td>\n      <td>Wednesday</td>\n      <td>426318</td>\n      <td>569605</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20175000168</td>\n      <td>1539602104</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Pending Request for Reservation Closure</td>\n      <td>1539602345</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n      <td>426412</td>\n      <td>426318</td>\n      <td>40504</td>\n      <td>Monday</td>\n      <td>241</td>\n      <td>143287</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20175000168</td>\n      <td>1539602345</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Pending Liquidation Request</td>\n      <td>1539745391</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n      <td>426653</td>\n      <td>241</td>\n      <td>40745</td>\n      <td>Monday</td>\n      <td>143046</td>\n      <td>143046</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20175000168</td>\n      <td>1539745391</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Request completed with account closure</td>\n      <td>1539745391</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n      <td>569699</td>\n      <td>143046</td>\n      <td>10991</td>\n      <td>Wednesday</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20175000642</td>\n      <td>1541672782</td>\n      <td>Bank Recess</td>\n      <td>1 - Client lost</td>\n      <td>Request created</td>\n      <td>1541672852</td>\n      <td>00624</td>\n      <td>APPLICANT</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37582</td>\n      <td>Thursday</td>\n      <td>70</td>\n      <td>578503</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:6]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:39.710957466Z",
     "start_time": "2023-06-19T10:53:39.709264205Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Base on the KPI create the label (or y) column.\n",
    "This column is maybe the KPI column for a given prefix? (? means not sure)\n",
    "### KPI Activity Occurrence\n",
    "TODO\n",
    "- Ask: If the activity occurs more than once in the trace, This code puts 0 from the 1st occurrence, but the OG code puts 0 from the last occurrence.\n",
    "       Which is the correct behavior?\n",
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def add_act_to_stop(df, activity_column_name=activity_column_name, activity_to_avoid=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame):\n",
    "        activity_to_avoid: A column of this activity name is added. Next, if an activity with this value is present\n",
    "                        in a trace, the trace rows, till the activity row, get a value of 1. This 1 is placed in\n",
    "                        the column labelled `activity_to_avoid`.\n",
    "    Returns:\n",
    "        df (pd.DataFrame):\n",
    "    \"\"\"\n",
    "    df[ activity_to_avoid ] = 0\n",
    "    gdf = df.groupby(case_id_name)\n",
    "    i = 0\n",
    "\n",
    "    for case_id, group in gdf:\n",
    "\n",
    "        # If activity_to_stop is in the trace\n",
    "        if activity_to_avoid in group[activity_column_name].to_list():\n",
    "            # print( activity_to_stop in group[activity_column_name].to_list() )\n",
    "            pass\n",
    "\n",
    "            # Get index of first occurrence of the activity_to_stop\n",
    "            # print( group.index[ group[activity_column_name] == activity_to_avoid ].values[0] )\n",
    "            index_of_first_occurrence = group.index[group[activity_column_name] == activity_to_avoid].values[0]\n",
    "\n",
    "            # Using group.loc makes sure the scope of assignment starts from the start of the trace only\n",
    "            # Rest of the dataframe is not affected.\n",
    "            # -1, cuz we don't want value of `1` in the row of activity_to_stop & with .loc the end is inclusive\n",
    "            group.loc[ :index_of_first_occurrence - 1, activity_to_avoid ] = 1\n",
    "            # print(\"==\", group.loc[ :index_of_first_occurrence - 1, : ] )\n",
    "\n",
    "            # using group.index is safer to access the main dataframe. This access the part of the main dataframe\n",
    "            # that corresponds to the group dataframe slice.\n",
    "            df.loc[ group.index, activity_to_avoid ] = group[activity_to_avoid]\n",
    "            # print( df.loc[ group.index, activity_to_stop ] )\n",
    "            # print( group.index )\n",
    "        i += 1\n",
    "        # if i == 30:\n",
    "        #     break\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:39.816000981Z",
     "start_time": "2023-06-19T10:53:39.715077385Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "if KPI == \"total_time\":\n",
    "    leadtime_per_case = df.groupby(case_id_name).agg(\"first\")[\"time_remaining\"]\n",
    "    df[\"lead_time\"] = df[case_id_name].map(leadtime_per_case)\n",
    "    target_column_name = 'lead_time'\n",
    "elif KPI == \"activity_occurrence\":\n",
    "    # In this case activity_to_avoid becomes the target (or y) column for the ML problem.\n",
    "    df = add_act_to_stop(df, activity_column_name=activity_column_name, activity_to_avoid=activity_to_avoid)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:48.105974787Z",
     "start_time": "2023-06-19T10:53:39.763544596Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def remove_two_step_traces(df):\n",
    "    \"\"\"\n",
    "    Removes cases with trace length of 2 originally, thus when time_remaining = 0 row is\n",
    "    removed the length becomes less than (lt) 2.\n",
    "    \"\"\"\n",
    "    gdf = df.groupby(case_id_name)\n",
    "    indexes_to_drop = []\n",
    "    # i = 0\n",
    "    for case_id, group in gdf:\n",
    "\n",
    "        if len(group) < 2:\n",
    "            indexes_to_drop.extend(group.index)\n",
    "\n",
    "        # i += 1\n",
    "        # if i == 4:\n",
    "        #     break\n",
    "\n",
    "    df = df.drop( indexes_to_drop ).reset_index(drop=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:48.149818103Z",
     "start_time": "2023-06-19T10:53:48.149492260Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ------ temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases with trace length le 2 are: 11 le & eq linux bash comparison operator convention\n",
      "Cases with trace length eq 3 are: 1761\n"
     ]
    }
   ],
   "source": [
    "gdf = df.groupby(case_id_name)\n",
    "indexes_to_drop = []\n",
    "# i = 0\n",
    "for case_id, group in gdf:\n",
    "    if len(group) <= 2:\n",
    "        # print( case_id )\n",
    "        indexes_to_drop.append(case_id)\n",
    "\n",
    "df_temp = pd.DataFrame(columns=df.columns)\n",
    "for case_id in indexes_to_drop:\n",
    "    df_small = df[df[case_id_name] == case_id].copy()\n",
    "    df_temp = pd.concat([df_temp, df_small], axis='rows')\n",
    "# le: less than or equal to\n",
    "# df_temp.to_csv(f\"{dataset}-size_le_2-cases.csv\", index=False)\n",
    "print( f\"Cases with trace length le 2 are: { len(indexes_to_drop) }\", \"le & eq linux bash comparison operator convention\" )\n",
    "\n",
    "\n",
    "gdf = df.groupby(case_id_name)\n",
    "indexes_to_drop = []\n",
    "# i = 0\n",
    "for case_id, group in gdf:\n",
    "    if len(group) == 3:\n",
    "        # print( case_id )\n",
    "        indexes_to_drop.append(case_id)\n",
    "\n",
    "df_temp = pd.DataFrame(columns=df.columns)\n",
    "for case_id in indexes_to_drop:\n",
    "    df_small = df[df[case_id_name] == case_id].copy()\n",
    "    df_temp = pd.concat([df_temp, df_small], axis='rows')\n",
    "# le: less than or equal to\n",
    "# df_temp.to_csv(f\"{dataset}-size_eq_3-cases.csv\", index=False)\n",
    "print( f\"Cases with trace length eq 3 are: { len(indexes_to_drop) }\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T12:19:06.843430894Z",
     "start_time": "2023-06-18T12:18:52.862450594Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ----------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases Before: 32,429\n",
      "Number of cases with length less than 2: 3,235\n",
      "Percentage of cases dropped: 9.975639088470196%\n",
      "Number of cases After: 29,194\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where remaining_time=0\n",
    "df = df.drop( df.index[ df[\"time_remaining\"] == 0 ] ).reset_index(drop=True)\n",
    "\n",
    "# Remove rows where with trace length < 2\n",
    "before_cases = len(df[case_id_name].unique())\n",
    "df = remove_two_step_traces(df)\n",
    "\n",
    "after_cases = len(df[case_id_name].unique())\n",
    "print(f\"Number of cases Before: {before_cases:,}\")\n",
    "print(f\"Number of cases with length less than 2: {(before_cases - after_cases):,}\")\n",
    "print(f\"Percentage of cases dropped: {( (before_cases - after_cases) / before_cases) * 100}%\")\n",
    "print(f\"Number of cases After: {after_cases:,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:48.616508417Z",
     "start_time": "2023-06-19T10:53:48.149703218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Not sure why the target_column is created and later on merged anyway\n",
    "# \"\"\"\n",
    "# target_column = df[target_column_name].copy()\n",
    "# df = df.drop(columns=[target_column_name, \"time_remaining\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:48.618451222Z",
     "start_time": "2023-06-19T10:53:48.617270434Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "    REQUEST_ID  START_DATE   CLOSURE_TYPE   CLOSURE_REASON   \n0  20175000168  1539175692  Client Recess  1 - Client lost  \\\n1  20175000168  1539175786  Client Recess  1 - Client lost   \n2  20175000168  1539602104  Client Recess  1 - Client lost   \n3  20175000168  1539602345  Client Recess  1 - Client lost   \n4  20175000642  1541672782    Bank Recess  1 - Client lost   \n\n                                            ACTIVITY    END_DATE  CE_UO   \n0  Service closure Request with network responsib...  1539175786  00044  \\\n1     Service closure Request with BO responsibility  1539602104    BOC   \n2            Pending Request for Reservation Closure  1539602345    BOC   \n3                        Pending Liquidation Request  1539745391    BOC   \n4                                    Request created  1541672852  00624   \n\n          ROLE  time_from_first  time_from_previous_et  time_from_midnight   \n0    APPLICANT                0                      0               46092  \\\n1  BACK-OFFICE               94                     94               46186   \n2  BACK-OFFICE           426412                 426318               40504   \n3  BACK-OFFICE           426653                    241               40745   \n4    APPLICANT                0                      0               37582   \n\n     weekday  activity_duration  time_remaining   \n0  Wednesday                 94          569699  \\\n1  Wednesday             426318          569605   \n2     Monday                241          143287   \n3     Monday             143046          143046   \n4   Thursday                 70          578503   \n\n   # ACTIVITY=Service closure Request with network responsibility   \n0                                                  0               \\\n1                                                  1                \n2                                                  1                \n3                                                  1                \n4                                                  0                \n\n   # ACTIVITY=Service closure Request with BO responsibility   \n0                                                  0          \\\n1                                                  0           \n2                                                  1           \n3                                                  1           \n4                                                  0           \n\n   # ACTIVITY=Pending Request for Reservation Closure   \n0                                                  0   \\\n1                                                  0    \n2                                                  0    \n3                                                  1    \n4                                                  0    \n\n   # ACTIVITY=Pending Liquidation Request   \n0                                       0  \\\n1                                       0   \n2                                       0   \n3                                       0   \n4                                       0   \n\n   # ACTIVITY=Request completed with account closure   \n0                                                  0  \\\n1                                                  0   \n2                                                  0   \n3                                                  0   \n4                                                  0   \n\n   # ACTIVITY=Request created  # ACTIVITY=Authorization Requested   \n0                           0                                   0  \\\n1                           0                                   0   \n2                           0                                   0   \n3                           0                                   0   \n4                           0                                   0   \n\n   # ACTIVITY=Evaluating Request (NO registered letter)   \n0                                                  0     \\\n1                                                  0      \n2                                                  0      \n3                                                  0      \n4                                                  0      \n\n   # ACTIVITY=Network Adjustment Requested   \n0                                        0  \\\n1                                        0   \n2                                        0   \n3                                        0   \n4                                        0   \n\n   # ACTIVITY=Pending Request for acquittance of heirs   \n0                                                  0    \\\n1                                                  0     \n2                                                  0     \n3                                                  0     \n4                                                  0     \n\n   # ACTIVITY=Request deleted  # ACTIVITY=Back-Office Adjustment Requested   \n0                           0                                            0  \\\n1                           0                                            0   \n2                           0                                            0   \n3                           0                                            0   \n4                           0                                            0   \n\n   # ACTIVITY=Evaluating Request (WITH registered letter)   \n0                                                  0       \\\n1                                                  0        \n2                                                  0        \n3                                                  0        \n4                                                  0        \n\n   # ACTIVITY=Request completed with customer recovery   \n0                                                  0    \\\n1                                                  0     \n2                                                  0     \n3                                                  0     \n4                                                  0     \n\n   # ACTIVITY=Pending Request for Network Information   \n0                                                  0   \\\n1                                                  0    \n2                                                  0    \n3                                                  0    \n4                                                  0    \n\n   Back-Office Adjustment Requested  \n0                                 0  \n1                                 0  \n2                                 0  \n3                                 0  \n4                                 0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>REQUEST_ID</th>\n      <th>START_DATE</th>\n      <th>CLOSURE_TYPE</th>\n      <th>CLOSURE_REASON</th>\n      <th>ACTIVITY</th>\n      <th>END_DATE</th>\n      <th>CE_UO</th>\n      <th>ROLE</th>\n      <th>time_from_first</th>\n      <th>time_from_previous_et</th>\n      <th>time_from_midnight</th>\n      <th>weekday</th>\n      <th>activity_duration</th>\n      <th>time_remaining</th>\n      <th># ACTIVITY=Service closure Request with network responsibility</th>\n      <th># ACTIVITY=Service closure Request with BO responsibility</th>\n      <th># ACTIVITY=Pending Request for Reservation Closure</th>\n      <th># ACTIVITY=Pending Liquidation Request</th>\n      <th># ACTIVITY=Request completed with account closure</th>\n      <th># ACTIVITY=Request created</th>\n      <th># ACTIVITY=Authorization Requested</th>\n      <th># ACTIVITY=Evaluating Request (NO registered letter)</th>\n      <th># ACTIVITY=Network Adjustment Requested</th>\n      <th># ACTIVITY=Pending Request for acquittance of heirs</th>\n      <th># ACTIVITY=Request deleted</th>\n      <th># ACTIVITY=Back-Office Adjustment Requested</th>\n      <th># ACTIVITY=Evaluating Request (WITH registered letter)</th>\n      <th># ACTIVITY=Request completed with customer recovery</th>\n      <th># ACTIVITY=Pending Request for Network Information</th>\n      <th>Back-Office Adjustment Requested</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20175000168</td>\n      <td>1539175692</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Service closure Request with network responsib...</td>\n      <td>1539175786</td>\n      <td>00044</td>\n      <td>APPLICANT</td>\n      <td>0</td>\n      <td>0</td>\n      <td>46092</td>\n      <td>Wednesday</td>\n      <td>94</td>\n      <td>569699</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20175000168</td>\n      <td>1539175786</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Service closure Request with BO responsibility</td>\n      <td>1539602104</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n      <td>94</td>\n      <td>94</td>\n      <td>46186</td>\n      <td>Wednesday</td>\n      <td>426318</td>\n      <td>569605</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20175000168</td>\n      <td>1539602104</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Pending Request for Reservation Closure</td>\n      <td>1539602345</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n      <td>426412</td>\n      <td>426318</td>\n      <td>40504</td>\n      <td>Monday</td>\n      <td>241</td>\n      <td>143287</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20175000168</td>\n      <td>1539602345</td>\n      <td>Client Recess</td>\n      <td>1 - Client lost</td>\n      <td>Pending Liquidation Request</td>\n      <td>1539745391</td>\n      <td>BOC</td>\n      <td>BACK-OFFICE</td>\n      <td>426653</td>\n      <td>241</td>\n      <td>40745</td>\n      <td>Monday</td>\n      <td>143046</td>\n      <td>143046</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20175000642</td>\n      <td>1541672782</td>\n      <td>Bank Recess</td>\n      <td>1 - Client lost</td>\n      <td>Request created</td>\n      <td>1541672852</td>\n      <td>00624</td>\n      <td>APPLICANT</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37582</td>\n      <td>Thursday</td>\n      <td>70</td>\n      <td>578503</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:48.662409083Z",
     "start_time": "2023-06-19T10:53:48.619377457Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split the Dataset for Machine Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to train and test the model with the following split:\n",
    "### For KPI Total Time\n",
    "* Train: 80% cases\n",
    "* Test: 20 % cases\n",
    "### For KPI Activity Occurrence\n",
    "- split on time, to avoid future leak in the testing data.\n",
    "- Make the time split such that cases with `activity_to_avoid` are 70% int trainset and 30% in testset.\n",
    "  - To do this, iteratively split and check percentage of `activity_to_avoid` cases in the trainset\n",
    "- Train: Mix the rest of the cases with 2/3 training mix\n",
    "- Test: Keep only the 1/3 testing mix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# if KPI == \"total_time\":\n",
    "#     cases = df[case_id_name].unique()\n",
    "#     train_n_valid_cases, test_cases = train_test_split(cases, train_size=0.67, test_size=0.33)\n",
    "#     train_cases, valid_cases = train_test_split(train_n_valid_cases, train_size=0.80)\n",
    "#\n",
    "#     df_train = df[df[case_id_name].isin(train_cases)].copy()\n",
    "#     df_train = df_train.reset_index(drop=True)\n",
    "#     df_valid = df[df[case_id_name].isin(valid_cases)].copy()\n",
    "#     df_valid = df_valid.reset_index(drop=True)\n",
    "#     df_test = df[df[case_id_name].isin(test_cases)].copy()\n",
    "#     df_test = df_test.reset_index(drop=True)\n",
    "#\n",
    "#     df_train.to_csv( os.path.join(preprocessed_data_dir, \"train-set-cfe.csv\"), index=False)\n",
    "#     df_test.to_csv( os.path.join(preprocessed_data_dir, \"test-set-cfe.csv\"), index=False)\n",
    "#\n",
    "# if KPI == \"activity_occurrence\":\n",
    "#     # === How much traces have `activity_to_avoid`\n",
    "#     case_ids_with_activity_to_avoid = []\n",
    "#     case_ids_without_activity_to_avoid = []\n",
    "#     gdf = df.groupby(case_id_name)\n",
    "#     for case_id, group in gdf:\n",
    "#         if activity_to_avoid in group[activity_column_name].to_list():\n",
    "#             case_ids_with_activity_to_avoid.append(case_id)\n",
    "#         else:\n",
    "#             case_ids_without_activity_to_avoid.append(case_id)\n",
    "#     print(f\"Cases with activity_to_avoid: {len(case_ids_with_activity_to_avoid):,}\")\n",
    "#     print(f\"Cases without activity_to_avoid: {len(case_ids_without_activity_to_avoid):,}\")\n",
    "#\n",
    "#     # === Split\n",
    "#     train_cases, test_cases = train_test_split(case_ids_with_activity_to_avoid, train_size=0.67, test_size=0.33)\n",
    "#\n",
    "#     # === Combine training split with rest of the cases\n",
    "#     train_cases = train_cases + case_ids_without_activity_to_avoid\n",
    "#     random.shuffle( train_cases )\n",
    "#\n",
    "#     df_train = df[df[case_id_name].isin(train_cases)].copy()\n",
    "#     df_train = df_train.reset_index(drop=True)\n",
    "#     df_test = df[df[case_id_name].isin(test_cases)].copy()\n",
    "#     df_test = df_test.reset_index(drop=True)\n",
    "#\n",
    "#     df_train.to_csv( os.path.join(preprocessed_data_dir, \"bank_acc_train.csv\"), index=False)\n",
    "#     df_test.to_csv( os.path.join(preprocessed_data_dir, \"bank_acc_test.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:53:48.662587641Z",
     "start_time": "2023-06-19T10:53:48.661668462Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Another type of Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def cases_with_activity_to_avoid(df, activity_column_name, activity_to_avoid):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        case_ids_with_activity_to_avoid, case_ids_without_activity_to_avoid\n",
    "    \"\"\"\n",
    "    # === How much traces have `activity_to_avoid`\n",
    "    case_ids_with_activity_to_avoid = []\n",
    "    case_ids_without_activity_to_avoid = []\n",
    "    gdf = df.groupby(case_id_name)\n",
    "    for case_id, group in gdf:\n",
    "        if activity_to_avoid in group[activity_column_name].to_list():\n",
    "            case_ids_with_activity_to_avoid.append(case_id)\n",
    "        else:\n",
    "            case_ids_without_activity_to_avoid.append(case_id)\n",
    "    # print(f\"Cases with activity_to_avoid: {len(case_ids_with_activity_to_avoid):,}\")\n",
    "    # print(f\"Cases without activity_to_avoid: {len(case_ids_without_activity_to_avoid):,}\")\n",
    "    return case_ids_with_activity_to_avoid, case_ids_without_activity_to_avoid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:55:04.937745963Z",
     "start_time": "2023-06-19T10:55:04.894670699Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 29194\n",
      "Total cases with activity_to_avoid: 3320\n",
      "Desired number of cases with activity_to_avoid in trainset: 2656.0\n",
      "Starting splitting procedure..\n",
      "length of start_end_couple: 29,194\n",
      "The min max range is 2017-05-29 08:47:46, 2019-03-04 10:56:42\n",
      "With length 55649336\n",
      "Threshold: 2018-12-25 13:15:34\n",
      "Condition for train_idxs = [201812008712   1545656766   1545657047] <= threshold\n",
      "Condition for test_idxs = [20179003319  1505220635  1548422992] >= threshold\n",
      "train_idxs after thrs = 19783\n",
      "train_idxs after thrs = 9411\n",
      "Split done\n",
      "Total cases: 29194\n",
      "Cases in df_train: 19783\n",
      "Cases in df_test: 9411\n",
      "Number of cases with activity_to_avoid in trainset: 2560 with train_size = 0.8\n",
      "Number of cases with activity_to_avoid in testset: 760 with train_size = 0.8\n"
     ]
    }
   ],
   "source": [
    "# Iteratively Check where to do the time split (what value for train_size to put)\n",
    "if KPI == \"activity_occurrence\":\n",
    "    def get_split_indexes(df, case_id_name, start_date_name, train_size=float):\n",
    "        print('Starting splitting procedure..')\n",
    "        start_end_couple = list()\n",
    "        for idx in df[case_id_name].unique():\n",
    "            df_ = df[df[case_id_name] == idx].reset_index(drop=True)\n",
    "            start_end_couple.append([idx, df_[start_date_name].values[0], df_[start_date_name].values[len(df_) - 1]])\n",
    "        start_end_couple = pd.DataFrame(start_end_couple, columns=['idx', 'start', 'end'])\n",
    "        print(f\"length of start_end_couple: {len(start_end_couple):,}\")  # temp\n",
    "        print(f\"The min max range is {pd.to_datetime( start_end_couple.start.min(), unit='s' )}, {pd.to_datetime( start_end_couple.end.max(), unit='s' )}\")\n",
    "        print(f\"With length { start_end_couple.end.max() - start_end_couple.start.min()}\")\n",
    "        # Initialize pdf of active cases and cdf of closed cases\n",
    "        # times_dict_pdf = dict()\n",
    "        # times_dict_cdf = dict()\n",
    "        # split = int((start_end_couple.end.max() - start_end_couple.start.min()) / 10_000)  # In order to get a 10000 dotted graph\n",
    "        # for time in range(int(start_end_couple.start.min()), int(start_end_couple.end.max()), split):\n",
    "        #     times_dict_pdf[time] = 0\n",
    "        #     times_dict_cdf[time] = 0\n",
    "        # for time in tqdm.tqdm(range(int(start_end_couple.start.min()), int(start_end_couple.end.max()), split)):\n",
    "        #     for line in np.array(start_end_couple[['start', 'end']]):\n",
    "        #         line = np.array(line)\n",
    "        #         if (line[0] <= time) and (line[1] >= time):\n",
    "        #             times_dict_pdf[time] += 1\n",
    "        # for time in tqdm.tqdm(range(int(start_end_couple.start.min()), int(start_end_couple.end.max()), split)):\n",
    "        #     for line in np.array(start_end_couple[['start', 'end']]):\n",
    "        #         line = np.array(line)\n",
    "        #         if (line[1] <= time):  # Keep just k closes cases\n",
    "        #             times_dict_cdf[time] += 1\n",
    "        # sns.set_style('darkgrid')\n",
    "        # plt.title('Number of active operations')\n",
    "        # plt.xlabel('Time')\n",
    "        # plt.ylabel('Count')\n",
    "\n",
    "        # times_pdf_df = pd.DataFrame( {\"time\": list(times_dict_pdf.keys()), \"count\": list(times_dict_pdf.values())} )\n",
    "        # times_cdf_df = pd.DataFrame( {\"time\": list(times_dict_cdf.keys()), \"count\": list(times_dict_cdf.values())} )\n",
    "        #\n",
    "        # sns.lineplot(data=times_pdf_df, x=\"time\", y=\"count\")\n",
    "        # sns.lineplot(data=times_cdf_df, x=\"time\", y=\"count\")\n",
    "        # plt.savefig('Active and completed cases distribution.png')\n",
    "        # times_dist = pd.DataFrame(columns=['times', 'pdf_active', 'cdf_closed'])\n",
    "        # times_dist['times'] = times_dict_pdf.keys()\n",
    "        # times_dist['pdf_active'] = times_dict_pdf.values()\n",
    "        # times_dist['cdf_closed'] = np.array(list(times_dict_cdf.values())) / (len(df[case_id_name].unique()))\n",
    "        # # Set threshold after 60 of closed activities (it'll be the train set)\n",
    "        # test_dim = times_dist[times_dist.cdf_closed > train_size].pdf_active.max()\n",
    "        # thrs = times_dist[times_dist.pdf_active == test_dim].times.values[0]\n",
    "\n",
    "        # ===== Manually modify the date to get the precious threshold value\n",
    "        date_string = \"2018-12-25 14:15:34\"\n",
    "        format_string = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "        # Convert the date string to a datetime object\n",
    "        datetime_obj = datetime.strptime(date_string, format_string)\n",
    "\n",
    "        # Convert the datetime object to an epoch timestamp\n",
    "        thrs = int( datetime_obj.timestamp() )\n",
    "\n",
    "        print(f\"Threshold: { pd.to_datetime(thrs, unit='s') }\")\n",
    "        print(f\"Condition for train_idxs = {start_end_couple[start_end_couple['end'] <= thrs].values[-1] } <= threshold\")\n",
    "        print(f\"Condition for test_idxs = {start_end_couple[start_end_couple['end'] >= thrs].values[0]} >= threshold\")\n",
    "        train_idxs = start_end_couple[start_end_couple['start'] <= thrs]['idx'].values\n",
    "        test_idxs = start_end_couple[start_end_couple['start'] >= thrs]['idx'].values\n",
    "\n",
    "        print(f\"train_idxs after thrs = {len(train_idxs)}\")  # temp\n",
    "        print(f\"train_idxs after thrs = {len(test_idxs)}\")  # temp\n",
    "\n",
    "        with open(f'indexes/train_idx_{case_id_name}.pkl', 'wb') as file_handle:\n",
    "            pickle.dump(train_idxs, file_handle)\n",
    "        with open(f'indexes/test_idx_{case_id_name}.pkl', 'wb') as file_handle:\n",
    "            pickle.dump(test_idxs, file_handle)\n",
    "        print('Split done')\n",
    "\n",
    "    print(f\"Total cases: {len(df[case_id_name].unique())}\")\n",
    "    case_ids_with_activity_to_avoid, _ = cases_with_activity_to_avoid(df, activity_column_name, activity_to_avoid)\n",
    "    print(f\"Total cases with activity_to_avoid: {len(case_ids_with_activity_to_avoid)}\")\n",
    "    print(f\"Desired number of cases with activity_to_avoid in trainset: {len(case_ids_with_activity_to_avoid) * 0.80 }\")\n",
    "\n",
    "    get_split_indexes(df, case_id_name, start_date_name, train_size=0.871)\n",
    "\n",
    "    with open(f'indexes/train_idx_{case_id_name}.pkl', 'rb') as file_handle:\n",
    "        train_idxs = pickle.load(file_handle)\n",
    "    with open(f'indexes/test_idx_{case_id_name}.pkl', 'rb') as file_handle:\n",
    "        test_idxs = pickle.load(file_handle)\n",
    "\n",
    "    df_train = df[df[case_id_name].isin(train_idxs)].copy()\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df[df[case_id_name].isin(test_idxs)].copy()\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    total_cases = len(df_train[case_id_name].unique()) + len(df_test[case_id_name].unique())\n",
    "    print(f\"Total cases: { total_cases }\")\n",
    "    print(f\"Cases in df_train: {len(df_train[case_id_name].unique())}\")\n",
    "    print(f\"Cases in df_test: {len(df_test[case_id_name].unique())}\")\n",
    "\n",
    "    case_ids_with_activity_to_avoid_train, _ = cases_with_activity_to_avoid(df_train, activity_column_name, activity_to_avoid)\n",
    "    case_ids_with_activity_to_avoid_test, _ = cases_with_activity_to_avoid(df_test, activity_column_name, activity_to_avoid)\n",
    "    print(f\"Number of cases with activity_to_avoid in trainset: {len(case_ids_with_activity_to_avoid_train)} with train_size = {train_size}\")\n",
    "    print(f\"Number of cases with activity_to_avoid in testset: {len(case_ids_with_activity_to_avoid_test)} with train_size = {train_size}\")\n",
    "    # Total case after: 27,780"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T10:55:22.101400904Z",
     "start_time": "2023-06-19T10:55:06.089561453Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 29194\n",
      "Cases in df_train: 28194\n",
      "Cases in df_test: 1000\n",
      "Number of cases with activity_to_avoid in trainset: 2560 with train_size = 0.8\n",
      "Number of cases with activity_to_avoid in testset: 760 with train_size = 0.8\n"
     ]
    }
   ],
   "source": [
    "# 9,411 test cases are too much, we'll keep the 760 important ones, which have the `activity_to_avoid` and 240 random cases.\n",
    "if KPI == \"activity_occurrence\":\n",
    "    # Sample 240 random cases from the testset\n",
    "    _, case_ids_without_activity_to_avoid_test = cases_with_activity_to_avoid(df_test, activity_column_name, activity_to_avoid)\n",
    "    test_cases_to_keep = random.sample(case_ids_without_activity_to_avoid_test, 240)\n",
    "\n",
    "    # remove these 240 cases from the case_ids_without_activity_to_avoid_test to get the remainder of the indexes to add to the train_idxs\n",
    "    remaining_test_cases = list(filter(lambda x: x not in test_cases_to_keep, case_ids_without_activity_to_avoid_test))\n",
    "\n",
    "    train_idxs = list(train_idxs)\n",
    "    test_idxs = list(test_idxs)\n",
    "    train_idxs += remaining_test_cases\n",
    "    test_idxs = case_ids_with_activity_to_avoid_test + test_cases_to_keep\n",
    "\n",
    "    # Sanity check to verify everything\n",
    "    df_train = df[df[case_id_name].isin(train_idxs)].copy()\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df[df[case_id_name].isin(test_idxs)].copy()\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    total_cases = len(df_train[case_id_name].unique()) + len(df_test[case_id_name].unique())\n",
    "    print(f\"Total cases: { total_cases }\")\n",
    "    print(f\"Cases in df_train: {len(df_train[case_id_name].unique())}\")\n",
    "    print(f\"Cases in df_test: {len(df_test[case_id_name].unique())}\")\n",
    "\n",
    "    case_ids_with_activity_to_avoid_train, _ = cases_with_activity_to_avoid(df_train, activity_column_name, activity_to_avoid)\n",
    "    case_ids_with_activity_to_avoid_test, _ = cases_with_activity_to_avoid(df_test, activity_column_name, activity_to_avoid)\n",
    "    print(f\"Number of cases with activity_to_avoid in trainset: {len(case_ids_with_activity_to_avoid_train)} with train_size = {train_size}\")\n",
    "    print(f\"Number of cases with activity_to_avoid in testset: {len(case_ids_with_activity_to_avoid_test)} with train_size = {train_size}\")\n",
    "\n",
    "    # If everything is okay save the dataframes (uncomment the below lines)\n",
    "    # df_train.to_csv( os.path.join(preprocessed_data_dir, \"bank_acc_train.csv\"))  #, index=False)\n",
    "    # df_test.to_csv( os.path.join(preprocessed_data_dir, \"bank_acc_test.csv\"))    #, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T11:02:26.370690117Z",
     "start_time": "2023-06-19T11:02:24.942577811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "if KPI == \"total_time\":\n",
    "    def get_split_indexes(df, case_id_name, start_date_name, train_size=float):\n",
    "        print('Starting splitting procedure..')\n",
    "        start_end_couple = list()\n",
    "        for idx in df[case_id_name].unique():\n",
    "            df_ = df[df[case_id_name] == idx].reset_index(drop=True)\n",
    "            start_end_couple.append([idx, df_[start_date_name].values[0], df_[start_date_name].values[len(df_) - 1]])\n",
    "        start_end_couple = pd.DataFrame(start_end_couple, columns=['idx', 'start', 'end'])\n",
    "        print(f\"length of start_end_couple: {len(start_end_couple):,}\")  # temp\n",
    "        print(f'The min max range is {pd.to_datetime( start_end_couple.start.min() )}, {pd.to_datetime( start_end_couple.end.max() )}')\n",
    "        print(f'With length {pd.to_datetime(start_end_couple.end.max() - start_end_couple.start.min() )}')\n",
    "        # Initialize pdf of active cases and cdf of closed cases\n",
    "        times_dict_pdf = dict()\n",
    "        times_dict_cdf = dict()\n",
    "        split = int((start_end_couple.end.max() - start_end_couple.start.min()) / 10000)  # In order to get a 10000 dotted graph\n",
    "        for time in range(int(start_end_couple.start.min()), int(start_end_couple.end.max()), split):\n",
    "            times_dict_pdf[time] = 0\n",
    "            times_dict_cdf[time] = 0\n",
    "        for time in tqdm.tqdm(range(int(start_end_couple.start.min()), int(start_end_couple.end.max()), split)):\n",
    "            for line in np.array(start_end_couple[['start', 'end']]):\n",
    "                line = np.array(line)\n",
    "                if (line[0] <= time) and (line[1] >= time):\n",
    "                    times_dict_pdf[time] += 1\n",
    "        for time in tqdm.tqdm(range(int(start_end_couple.start.min()), int(start_end_couple.end.max()), split)):\n",
    "            for line in np.array(start_end_couple[['start', 'end']]):\n",
    "                line = np.array(line)\n",
    "                if (line[1] <= time):  # Keep just k closes cases\n",
    "                    times_dict_cdf[time] += 1\n",
    "        sns.set_style('darkgrid')\n",
    "        plt.title('Number of active operations')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        times_pdf_df = pd.DataFrame( {\"time\": list(times_dict_pdf.keys()), \"count\": list(times_dict_pdf.values())} )\n",
    "        times_cdf_df = pd.DataFrame( {\"time\": list(times_dict_cdf.keys()), \"count\": list(times_dict_cdf.values())} )\n",
    "\n",
    "        sns.lineplot(data=times_pdf_df, x=\"time\", y=\"count\")\n",
    "        sns.lineplot(data=times_cdf_df, x=\"time\", y=\"count\")\n",
    "        plt.savefig('Active and completed cases distribution.png')\n",
    "        times_dist = pd.DataFrame(columns=['times', 'pdf_active', 'cdf_closed'])\n",
    "        times_dist['times'] = times_dict_pdf.keys()\n",
    "        times_dist['pdf_active'] = times_dict_pdf.values()\n",
    "        times_dist['cdf_closed'] = np.array(list(times_dict_cdf.values())) / (len(df[case_id_name].unique()))\n",
    "        # Set threshold after 60 of closed activities (it'll be the train set)\n",
    "        test_dim = times_dist[times_dist.cdf_closed > train_size].pdf_active.max()\n",
    "        thrs = times_dist[times_dist.pdf_active == test_dim].times.values[0]\n",
    "\n",
    "        print(f\"Threshold: {thrs}\")\n",
    "        print(f\"Condition for train_idxs = {pd.to_datetime(start_end_couple['end'])} <= {thrs}\")\n",
    "        print(f\"Condition for test_idxs = {pd.to_datetime(start_end_couple['end'])} >= {thrs} and {pd.to_datetime(start_end_couple['start'])} <= {thrs}\")\n",
    "        train_idxs = start_end_couple[start_end_couple['end'] <= thrs]['idx'].values\n",
    "        test_idxs = start_end_couple[start_end_couple['end'] >= thrs][start_end_couple['start'] <= thrs]['idx'].values\n",
    "\n",
    "        print(f\"train_idxs after thrs = {len(train_idxs)}\")  # temp\n",
    "        print(f\"train_idxs after thrs = {len(test_idxs)}\")  # temp\n",
    "\n",
    "        with open(f'indexes/train_idx_{case_id_name}.pkl', 'wb') as file_handle:\n",
    "            pickle.dump(train_idxs, file_handle)\n",
    "        with open(f'indexes/test_idx_{case_id_name}.pkl', 'wb') as file_handle:\n",
    "            pickle.dump(test_idxs, file_handle)\n",
    "        print('Split done')\n",
    "\n",
    "    print(f\"Total cases: {len(df[case_id_name].unique())}\")\n",
    "\n",
    "    get_split_indexes(df, case_id_name, start_date_name, train_size=train_size)\n",
    "\n",
    "    with open(f'indexes/train_idx_{case_id_name}.pkl', 'rb') as file_handle:\n",
    "        train_idxs = pickle.load(file_handle)\n",
    "    with open(f'indexes/test_idx_{case_id_name}.pkl', 'rb') as file_handle:\n",
    "        test_idxs = pickle.load(file_handle)\n",
    "\n",
    "    total = len(train_idxs) + len(test_idxs)\n",
    "    print(f\"Percentage of train_idxs: { (len(train_idxs) / total):,}%, Number of train_idxs: {len(train_idxs)}\")\n",
    "    print(f\"Percentage of test_idxs: { len(test_idxs) / total:,}%, Number of test_idxs: {len(test_idxs)}\")\n",
    "\n",
    "    df_train = df[df[case_id_name].isin(train_idxs)].copy()\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df[df[case_id_name].isin(test_idxs)].copy()\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    print(f\"Total cases: { ( len(df_train[case_id_name].unique()) + len(df_test[case_id_name].unique()) ) }\")\n",
    "    print(f\"Rows in df_train: {len(df_train)}, Cases in df_train: {len(df_train[case_id_name].unique()) }\")\n",
    "    print(f\"Rows in df_test: {len(df_test)}, Cases in df_test: {len(df_test[case_id_name].unique()) }\")\n",
    "\n",
    "    df_train.to_csv( os.path.join(preprocessed_data_dir, \"vinst_train.csv\"), index=False)\n",
    "    df_test.to_csv( os.path.join(preprocessed_data_dir, \"vinst_test.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T18:26:52.672825920Z",
     "start_time": "2023-06-18T18:26:52.665383494Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "case_id_name = 'SR_Number'  # The case identifier column name.\n",
    "start_date_name = 'Change_Date+Time'  # Maybe change to start_et (start even time)\n",
    "activity_column_name = \"ACTIVITY\"\n",
    "\n",
    "vinst_train = pd.read_csv( os.path.join(preprocessed_data_dir, \"vinst_train.csv\") )\n",
    "vinst_test = pd.read_csv( os.path.join(preprocessed_data_dir, \"vinst_test.csv\") )\n",
    "\n",
    "vinst_train[start_date_name] = pd.to_datetime( vinst_train[start_date_name], unit='s')\n",
    "vinst_test[start_date_name] = pd.to_datetime( vinst_test[start_date_name], unit='s')\n",
    "\n",
    "vinst_train.to_csv( os.path.join(preprocessed_data_dir, \"vinst_train-temp.csv\"), index=False)\n",
    "vinst_test.to_csv( os.path.join(preprocessed_data_dir, \"vinst_test-temp.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T19:07:35.681172944Z",
     "start_time": "2023-06-18T19:07:35.003028030Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check REquest_ID indexes from OG project"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 24566\n",
      "Cases in df_train: 21708\n",
      "Cases in df_test: 2858\n"
     ]
    }
   ],
   "source": [
    "with open(f'indexes/train_idx_{case_id_name}.pkl', 'rb') as file_handle:\n",
    "    train_idxs = pickle.load(file_handle)\n",
    "with open(f'indexes/test_idx_{case_id_name}.pkl', 'rb') as file_handle:\n",
    "    test_idxs = pickle.load(file_handle)\n",
    "\n",
    "total_cases = len(df_train[case_id_name].unique()) + len(df_test[case_id_name].unique())\n",
    "print(f\"Total cases: { total_cases }\")\n",
    "print(f\"Cases in df_train: {len(df_train[case_id_name].unique())}\")\n",
    "print(f\"Cases in df_test: {len(df_test[case_id_name].unique())}\")\n",
    "\n",
    "df_train = df[df[case_id_name].isin(train_idxs)].copy()\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df[df[case_id_name].isin(test_idxs)].copy()\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "df_train[start_date_name] = pd.to_datetime( df_train[start_date_name], unit='s')\n",
    "df_test[start_date_name] = pd.to_datetime( df_test[start_date_name], unit='s')\n",
    "\n",
    "df_train.to_csv( os.path.join(preprocessed_data_dir, \"train-temp.csv\"), index=False)\n",
    "df_test.to_csv( os.path.join(preprocessed_data_dir, \"test-temp.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T22:13:39.716983641Z",
     "start_time": "2023-06-18T22:13:38.867141964Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the Preprocessed dataset for later use"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict using CatBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "# First we need to get all the categorical columns for the CatBoost Algorithm\n",
    "categorical_features = df.iloc[:, 1:-1].select_dtypes(exclude=np.number).columns\n",
    "df[categorical_features] = df[categorical_features].astype(str)\n",
    "categorical_features = categorical_features.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T16:40:10.228897578Z",
     "start_time": "2023-05-23T16:40:10.181538565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, 1: -1]\n",
    "y_train = df_train.iloc[:, -1 ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, y_train, cat_features=categorical_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "params = {\n",
    "    'depth': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'iterations': 3000,\n",
    "    'early_stopping_rounds': 5,\n",
    "    'thread_count': 4,\n",
    "    'logging_level': 'Silent',\n",
    "    'task_type': \"CPU\"  # \"GPU\" if int(os.environ[\"USE_GPU\"]) else \"CPU\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(**params)\n",
    "if use_saved_model:\n",
    "    model.load_model(fname=\"catboost_model.cbm\", format='cbm')\n",
    "else:\n",
    "    model.fit(train_pool)\n",
    "    model.save_model(fname=\"catboost_model.cbm\", format=\"cbm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "X_train_pool = Pool(X_train, cat_features=categorical_features)\n",
    "y_train_pred = model.predict(X_train_pool)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "24373.989614527192"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31219\n",
      "31219\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_series = pd.DataFrame(y_train_pred, columns=[\"Y_predictions\"])\n",
    "print(len(y_train_pred_series))\n",
    "print(len(df_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# see the results\n",
    "# # Should be a function\n",
    "df_train_info = df_train[[case_id_name, \"Status\", activity_column_name, \"time_from_first\", \"time_from_previous_et\",\n",
    "                    \"time_remaining\", target_column_name]].copy()\n",
    "# Convert seconds to hours\n",
    "df_train_info.loc[:, \"time_remaining\"] = round( df_train_info.loc[:, \"time_remaining\"] / (60 * 60), 2)\n",
    "df_train_info[target_column_name] = df_train_info[target_column_name] / (60 * 60)\n",
    "y_train_pred_series = round( y_train_pred_series / (60 * 60), 2)\n",
    "\n",
    "df_results = pd.concat([df_train_info, y_train_pred_series], axis=\"columns\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "df_results.to_csv(\"temp.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
